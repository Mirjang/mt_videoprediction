----------------- Options ---------------
               batch_size: 1                             
                    beta1: 0.5                           
          checkpoints_dir: /mnt/raid/patrickradner/checkpoints	[default: ../checkpoints]
               clip_grads: 0.25                          	[default: 1]
               clips_file: info.csv                      
           continue_train: False                         
                 dataroot: /mnt/raid/patrickradner/datasets/yt//river_relaxing	[default: ../datasets]
             dataset_mode: video                         
              display_env: river_relaxing_lhc_lhc_128    	[default: main]
             display_freq: 50                            	[default: 400]
               display_id: 1                             
            display_ncols: 4                             
             display_port: 8197                          	[default: 8097]
           display_server: http://localhost              
          display_winsize: 256                           
       dvd_spatial_frames: 8                             
  dvd_temporal_downsample: 2                             
                    epoch: latest                        
              epoch_count: 1                             
                      fps: 25                            	[default: 30]
                generator: lhc                           	[default: dvdgansimple]
                  gpu_ids: -1                            	[default: 0]
                init_gain: 0.02                          
                init_type: xavier                        
                 input_nc: 3                             
                  isTrain: True                          	[default: None]
                lambda_GP: 1                             
                lambda_L1: 20.0                          	[default: 10.0]
                 lambda_S: 1.0                           	[default: 0.1]
                 lambda_T: 1.0                           	[default: 0.1]
                load_iter: 0                             	[default: 0]
                 lossType: L1                            
                       lr: 0.0001                        	[default: 0.0004]
           lr_decay_iters: 50                            
                lr_policy: lambda                        
          max_clip_length: 1.0                           	[default: 2.0]
         max_dataset_size: inf                           
     max_per_frame_losses: 10                            
     max_val_dataset_size: 500                           	[default: 1000]
                    model: lhc                           	[default: simpleVideo]
                 n_critic: 2                             
                     name: river_relaxing_lhc_lhc_128    	[default: experiment_name]
                      ndf: 64                            
                      ngf: 64                            
                    niter: 250                           	[default: 100]
              niter_decay: 250                           	[default: 100]
                  no_html: False                         
                 no_lsgan: True                          
                     norm: instance                      
                     nrdf: 64                            
                     nref: 32                            
                     nrhf: 64                            
       num_display_frames: 16                            	[default: 8]
              num_threads: 4                             
                output_nc: 3                             
                    phase: train                         
                pool_size: 0                             
          pretrain_epochs: 0                             
               print_freq: 50                            	[default: 100]
             reparse_data: False                         
           resize_or_crop: resize_and_crop               
               resolution: 128                           	[default: 64]
             sanity_check: False                         
             save_by_iter: False                         
          save_epoch_freq: 5                             
         save_latest_freq: 5000                          
           serial_batches: False                         
              skip_frames: 1                             
                   suffix:                               
                      tld: 0.8                           	[default: 0.5]
                      tlg: 0.2                           	[default: 0.5]
               train_mode: mixed                         	[default: frame]
            unroll_frames: 1                             
         update_html_freq: 50                            	[default: 1000]
          validation_freq: 5                             	[default: 10]
           validation_set: split                         	[default: test]
                  verbose: True                          	[default: False]
             weight_decay: 0                             
----------------- End -------------------
dataset [BaseDataset] was created
#training samples = 7186
#validation samples = 500
cpu
initialize network with xavier
initialize network with xavier
initialize network with xavier
model [DHCModel] was created
---------- Networks initialized -------------
LHC(
  (encoder): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)
    (1): ReLU(inplace=True)
    (2): AvgPool2d(kernel_size=2, stride=2, padding=0)
    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)
    (4): ReLU(inplace=True)
    (5): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)
    (6): ReLU(inplace=True)
    (7): AdaptiveAvgPool2d(output_size=(32, 32))
  )
  (rule_mlp): Sequential(
    (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    (1): ReLU()
    (2): Conv1d(32, 64, kernel_size=(1,), stride=(1,))
    (3): ReLU()
  )
  (velocity_mlp): Sequential(
    (0): Conv1d(64, 32, kernel_size=(1,), stride=(1,))
    (1): ReLU()
    (2): Conv1d(32, 2, kernel_size=(1,), stride=(1,))
    (3): Tanh()
  )
  (decoder): Sequential(
    (0): Upsample(scale_factor=2.0, mode=nearest)
    (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)
    (2): ReLU(inplace=True)
    (3): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)
    (4): ReLU(inplace=True)
    (5): Upsample(scale_factor=2.0, mode=nearest)
    (6): Conv2d(32, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), padding_mode=reflect)
  )
)
[Network netG] Total number of parameters : 0.119 M
SpatialDiscriminator(
  (pre_conv): Sequential(
    (0): SpectralNorm(
      (module): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (1): ReLU()
    (2): SpectralNorm(
      (module): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    )
    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)
  )
  (pre_skip): SpectralNorm(
    (module): Conv2d(3, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (conv1): GBlock(
    (conv0): SpectralNorm(
      (module): Conv2d(16, 32, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    )
    (conv1): SpectralNorm(
      (module): Conv2d(32, 32, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
    )
    (conv_sc): SpectralNorm(
      (module): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))
    )
  )
  (attn): SelfAttention(
    (query_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
    (key_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
    (value_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    (softmax): Softmax(dim=-1)
  )
  (conv2): Sequential(
    (0): GBlock(
      (conv0): SpectralNorm(
        (module): Conv2d(32, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv1): SpectralNorm(
        (module): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv_sc): SpectralNorm(
        (module): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (1): GBlock(
      (conv0): SpectralNorm(
        (module): Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv1): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv_sc): SpectralNorm(
        (module): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (2): GBlock(
      (conv0): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv1): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv_sc): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (linear): SpectralNorm(
    (module): Linear(in_features=128, out_features=1, bias=True)
  )
)
[Network netDs] Total number of parameters : 0.625 M
TemporalDiscriminator(
  (pre_conv): Sequential(
    (0): SpectralNorm(
      (module): Conv3d(3, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    )
    (1): ReLU()
    (2): SpectralNorm(
      (module): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))
    )
    (3): AvgPool3d(kernel_size=2, stride=2, padding=0)
  )
  (pre_skip): SpectralNorm(
    (module): Conv3d(3, 16, kernel_size=(1, 1, 1), stride=(1, 1, 1))
  )
  (res3d): Res3dBlock(
    (conv0): SpectralNorm(
      (module): Conv3d(16, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=(1, 1, 1))
    )
    (conv1): SpectralNorm(
      (module): Conv3d(32, 32, kernel_size=[3, 3, 3], stride=(1, 1, 1), padding=(1, 1, 1))
    )
    (conv_sc): SpectralNorm(
      (module): Conv3d(16, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))
    )
  )
  (self_attn): SelfAttention(
    (query_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
    (key_conv): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))
    (value_conv): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))
    (softmax): Softmax(dim=-1)
  )
  (conv): Sequential(
    (0): GBlock(
      (conv0): SpectralNorm(
        (module): Conv2d(32, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv1): SpectralNorm(
        (module): Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv_sc): SpectralNorm(
        (module): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (1): GBlock(
      (conv0): SpectralNorm(
        (module): Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv1): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv_sc): SpectralNorm(
        (module): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1))
      )
    )
    (2): GBlock(
      (conv0): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv1): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))
      )
      (conv_sc): SpectralNorm(
        (module): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
      )
    )
  )
  (linear): SpectralNorm(
    (module): Linear(in_features=128, out_features=1, bias=True)
  )
)
[Network netDt] Total number of parameters : 0.659 M
-----------------------------------------------
create web directory /mnt/raid/patrickradner/checkpoints/river_relaxing_lhc_lhc_128/web...
----------------0-----------
x:  0.0 3.4686036087805405e-06
v:  -1.674827811160018e-13 -3.3390323383834797e-14
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------1-----------
x:  0.0 8.319753219687698e-10
v:  -3.355133569148991e-17 5.3201229518368796e-18
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------2-----------
x:  0.0 1.637541285296612e-13
v:  -6.673078047276598e-21 -2.341250309403599e-22
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------3-----------
x:  0.0 3.013761755176522e-17
v:  -1.0061965219519924e-24 -6.162766897158788e-26
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------4-----------
x:  0.0 5.784908212625556e-21
v:  -2.0445465252145874e-28 -4.6365638698127836e-30
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------5-----------
x:  0.0 1.0121472941905271e-24
v:  -3.7151290307491126e-32 -2.8376321589544708e-33
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------6-----------
x:  0.0 1.738259386145123e-28
v:  -5.459867054488502e-36 -5.993634688441125e-37
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------7-----------
x:  0.0 3.222234551620933e-32
v:  -1.0430999521725717e-39 -1.3857160254015251e-40
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------8-----------
x:  0.0 5.3555785215585246e-36
v:  -1.9057659114817512e-43 -2.2420775429197073e-44
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------9-----------
x:  0.0 8.85343172357348e-40
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------10-----------
x:  0.0 1.6675451725465323e-43
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------11-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------12-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------13-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------14-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------15-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------16-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------17-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------18-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------19-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------20-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------21-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------22-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------23-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
tensor(0) tensor(-1., grad_fn=<MinBackward1>) tensor(1., grad_fn=<MaxBackward1>)
Ds: mean: 0.06701960	norm: 4.15403948
Dt: mean: 0.00740344	norm: 0.36322724
river_relaxing_lhc_lhc_128
(epoch: 1, iters: 0, time: 5.784, data: 0.266)	Gs: 0.000000 Gt: 0.000000 Ds: 0.900314 Dt: 0.714503 Ds_real: 0.159435 Ds_fake: 1.641194 Dt_real: 0.940404 Dt_fake: 0.488602 accDs_real: 0.852625 accDs_fake: 0.193749 accDt_real: 0.390494 accDt_fake: 0.613531 
----------------0-----------
x:  0.0 2.832841801136965e-06
v:  -1.5419939359672535e-13 9.594680159695414e-15
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------1-----------
x:  0.0 6.585250678092791e-10
v:  -3.003161931935022e-17 6.297176697265634e-18
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------2-----------
x:  0.0 1.301629431408463e-13
v:  -5.6444668386634036e-21 2.7116284561227886e-22
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------3-----------
x:  0.0 2.4905359378954946e-17
v:  -7.607074455898054e-25 7.639429462666178e-26
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------4-----------
x:  0.0 4.544337222703834e-21
v:  -1.5647031258710735e-28 -1.8391201567767502e-30
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------5-----------
x:  0.0 7.995400604216347e-25
v:  -2.7256892809126874e-32 -5.279908741693447e-34
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------6-----------
x:  0.0 1.405992896573333e-28
v:  -4.525901493491359e-36 -1.5389146055200543e-37
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------7-----------
x:  0.0 2.4412941449681772e-32
v:  -7.469439295283075e-40 -3.887902589269205e-41
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------8-----------
x:  0.0 4.1563224535701706e-36
v:  -1.471363387541058e-43 -4.203895392974451e-45
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------9-----------
x:  0.0 7.0591811438826985e-40
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------10-----------
x:  0.0 1.2191296639625909e-43
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------11-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------12-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------13-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------14-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------15-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------16-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------17-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------18-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------19-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------20-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------21-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------22-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
----------------23-----------
x:  0.0 0.0
v:  0.0 0.0
p:  0.0 0.0 0.0 0.0
d:  0.0 0.0 
k:  0.022097086533904076 0.022097086533904076 
tensor(0) tensor(-1., grad_fn=<MinBackward1>) tensor(0.9973, grad_fn=<MaxBackward1>)
Ds: mean: 0.01700731	norm: 0.83911001
Dt: mean: 0.00393052	norm: 0.15743783
netG: mean: 0.00599789	norm: 0.05159540
----------------0-----------
x:  0.0 3.372191486050724e-06
v:  -7.551627589919008e-08 -6.719174194813604e-08
p:  -1.1920928955078125e-07 -5.960464477539063e-08 3.552713678800501e-15 1.4210854715202004e-14
d:  7.105427357601002e-15 2.842170943040401e-14 
k:  0.022097086533904076 0.022097086533904076 
----------------1-----------
x:  0.0 0.00010030790144810453
v:  -7.551975045316794e-08 -6.719081824257955e-08
p:  -2.384185791015625e-07 -1.1920928955078125e-07 1.4210854715202004e-14 5.684341886080802e-14
d:  2.842170943040401e-14 1.1368683772161603e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------2-----------
x:  0.0 0.00010030366684077308
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -3.5762786865234375e-07 -1.7881393432617188e-07 3.197442310920451e-14 1.2789769243681803e-13
d:  6.394884621840902e-14 2.5579538487363607e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------3-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -4.76837158203125e-07 -2.384185791015625e-07 5.684341886080802e-14 2.2737367544323206e-13
d:  1.1368683772161603e-13 4.547473508864641e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------4-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -5.960464477539062e-07 -2.980232238769531e-07 8.881784197001252e-14 3.552713678800501e-13
d:  1.7763568394002505e-13 7.105427357601002e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------5-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -7.152557373046875e-07 -3.5762786865234375e-07 1.2789769243681803e-13 5.115907697472721e-13
d:  2.5579538487363607e-13 1.0231815394945443e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------6-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -8.344650268554688e-07 -4.172325134277344e-07 1.7408297026122455e-13 6.963318810448982e-13
d:  3.481659405224491e-13 1.3926637620897964e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------7-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -9.5367431640625e-07 -4.76837158203125e-07 2.2737367544323206e-13 9.094947017729282e-13
d:  4.547473508864641e-13 1.8189894035458565e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------8-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.0728836059570312e-06 -5.364418029785156e-07 2.877698079828406e-13 1.1510792319313623e-12
d:  5.755396159656812e-13 2.3021584638627246e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------9-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.1920928955078125e-06 -5.960464477539062e-07 3.552713678800501e-13 1.4210854715202004e-12
d:  7.105427357601002e-13 2.8421709430404007e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------10-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.3113021850585938e-06 -6.556510925292969e-07 4.298783551348606e-13 1.7195134205394424e-12
d:  8.597567102697212e-13 3.439026841078885e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------11-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.430511474609375e-06 -7.152557373046875e-07 5.115907697472721e-13 2.0463630789890885e-12
d:  1.0231815394945443e-12 4.092726157978177e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------12-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.5497207641601562e-06 -7.748603820800781e-07 6.004086117172847e-13 2.4016344468691386e-12
d:  1.2008172234345693e-12 4.803268893738277e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------13-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.6689300537109375e-06 -8.344650268554688e-07 6.963318810448982e-13 2.7853275241795927e-12
d:  1.3926637620897964e-12 5.5706550483591855e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------14-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.7881393432617188e-06 -8.940696716308594e-07 7.993605777301127e-13 3.197442310920451e-12
d:  1.5987211554602254e-12 6.394884621840902e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------15-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.9073486328125e-06 -9.5367431640625e-07 9.094947017729282e-13 3.637978807091713e-12
d:  1.8189894035458565e-12 7.275957614183426e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------16-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.0265579223632812e-06 -1.0132789611816406e-06 1.0267342531733448e-12 4.106937012693379e-12
d:  2.0534685063466895e-12 8.213874025386758e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------17-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.1457672119140625e-06 -1.0728836059570312e-06 1.1510792319313623e-12 4.604316927725449e-12
d:  2.3021584638627246e-12 9.208633855450898e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------18-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.2649765014648438e-06 -1.1324882507324219e-06 1.2825296380469808e-12 5.130118552187923e-12
d:  2.5650592760939617e-12 1.0260237104375847e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------19-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.384185791015625e-06 -1.1920928955078125e-06 1.4210854715202004e-12 5.6843418860808015e-12
d:  2.8421709430404007e-12 1.1368683772161603e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------20-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.5033950805664062e-06 -1.2516975402832031e-06 1.566746732351021e-12 6.266986929404084e-12
d:  3.133493464702042e-12 1.2533973858808167e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------21-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.6226043701171875e-06 -1.3113021850585938e-06 1.7195134205394424e-12 6.87805368215777e-12
d:  3.439026841078885e-12 1.375610736431554e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------22-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.7418136596679688e-06 -1.3709068298339844e-06 1.879385536085465e-12 7.51754214434186e-12
d:  3.75877107217093e-12 1.503508428868372e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------23-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.86102294921875e-06 -1.430511474609375e-06 2.0463630789890885e-12 8.185452315956354e-12
d:  4.092726157978177e-12 1.6370904631912708e-11 
k:  0.022097086533904076 0.022097086533904076 
tensor(0) tensor(-1., grad_fn=<MinBackward1>) tensor(1., grad_fn=<MaxBackward1>)
Ds: mean: 0.00904833	norm: 0.46461545
Dt: mean: 0.00275324	norm: 0.09443368
----------------0-----------
x:  0.0 4.295063263271004e-06
v:  -7.551631853175422e-08 -6.719167799928982e-08
p:  -1.1920928955078125e-07 -5.960464477539063e-08 3.552713678800501e-15 1.4210854715202004e-14
d:  7.105427357601002e-15 2.842170943040401e-14 
k:  0.022097086533904076 0.022097086533904076 
----------------1-----------
x:  0.0 0.0001003079887595959
v:  -7.551975045316794e-08 -6.719081824257955e-08
p:  -2.384185791015625e-07 -1.1920928955078125e-07 1.4210854715202004e-14 5.684341886080802e-14
d:  2.842170943040401e-14 1.1368683772161603e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------2-----------
x:  0.0 0.00010030366684077308
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -3.5762786865234375e-07 -1.7881393432617188e-07 3.197442310920451e-14 1.2789769243681803e-13
d:  6.394884621840902e-14 2.5579538487363607e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------3-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -4.76837158203125e-07 -2.384185791015625e-07 5.684341886080802e-14 2.2737367544323206e-13
d:  1.1368683772161603e-13 4.547473508864641e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------4-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -5.960464477539062e-07 -2.980232238769531e-07 8.881784197001252e-14 3.552713678800501e-13
d:  1.7763568394002505e-13 7.105427357601002e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------5-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -7.152557373046875e-07 -3.5762786865234375e-07 1.2789769243681803e-13 5.115907697472721e-13
d:  2.5579538487363607e-13 1.0231815394945443e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------6-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -8.344650268554688e-07 -4.172325134277344e-07 1.7408297026122455e-13 6.963318810448982e-13
d:  3.481659405224491e-13 1.3926637620897964e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------7-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -9.5367431640625e-07 -4.76837158203125e-07 2.2737367544323206e-13 9.094947017729282e-13
d:  4.547473508864641e-13 1.8189894035458565e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------8-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.0728836059570312e-06 -5.364418029785156e-07 2.877698079828406e-13 1.1510792319313623e-12
d:  5.755396159656812e-13 2.3021584638627246e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------9-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.1920928955078125e-06 -5.960464477539062e-07 3.552713678800501e-13 1.4210854715202004e-12
d:  7.105427357601002e-13 2.8421709430404007e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------10-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.3113021850585938e-06 -6.556510925292969e-07 4.298783551348606e-13 1.7195134205394424e-12
d:  8.597567102697212e-13 3.439026841078885e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------11-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.430511474609375e-06 -7.152557373046875e-07 5.115907697472721e-13 2.0463630789890885e-12
d:  1.0231815394945443e-12 4.092726157978177e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------12-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.5497207641601562e-06 -7.748603820800781e-07 6.004086117172847e-13 2.4016344468691386e-12
d:  1.2008172234345693e-12 4.803268893738277e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------13-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.6689300537109375e-06 -8.344650268554688e-07 6.963318810448982e-13 2.7853275241795927e-12
d:  1.3926637620897964e-12 5.5706550483591855e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------14-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.7881393432617188e-06 -8.940696716308594e-07 7.993605777301127e-13 3.197442310920451e-12
d:  1.5987211554602254e-12 6.394884621840902e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------15-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -1.9073486328125e-06 -9.5367431640625e-07 9.094947017729282e-13 3.637978807091713e-12
d:  1.8189894035458565e-12 7.275957614183426e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------16-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.0265579223632812e-06 -1.0132789611816406e-06 1.0267342531733448e-12 4.106937012693379e-12
d:  2.0534685063466895e-12 8.213874025386758e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------17-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.1457672119140625e-06 -1.0728836059570312e-06 1.1510792319313623e-12 4.604316927725449e-12
d:  2.3021584638627246e-12 9.208633855450898e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------18-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.2649765014648438e-06 -1.1324882507324219e-06 1.2825296380469808e-12 5.130118552187923e-12
d:  2.5650592760939617e-12 1.0260237104375847e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------19-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.384185791015625e-06 -1.1920928955078125e-06 1.4210854715202004e-12 5.6843418860808015e-12
d:  2.8421709430404007e-12 1.1368683772161603e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------20-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.5033950805664062e-06 -1.2516975402832031e-06 1.566746732351021e-12 6.266986929404084e-12
d:  3.133493464702042e-12 1.2533973858808167e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------21-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.6226043701171875e-06 -1.3113021850585938e-06 1.7195134205394424e-12 6.87805368215777e-12
d:  3.439026841078885e-12 1.375610736431554e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------22-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.7418136596679688e-06 -1.3709068298339844e-06 1.879385536085465e-12 7.51754214434186e-12
d:  3.75877107217093e-12 1.503508428868372e-11 
k:  0.022097086533904076 0.022097086533904076 
----------------23-----------
x:  0.0 0.00010030367411673069
v:  -7.551974334774059e-08 -6.71908111371522e-08
p:  -2.86102294921875e-06 -1.430511474609375e-06 2.0463630789890885e-12 8.185452315956354e-12
d:  4.092726157978177e-12 1.6370904631912708e-11 
k:  0.022097086533904076 0.022097086533904076 
tensor(0) tensor(-1., grad_fn=<MinBackward1>) tensor(1., grad_fn=<MaxBackward1>)
Ds: mean: 0.00726800	norm: 0.33995406
Dt: mean: 0.00282573	norm: 0.09630448
netG: mean: 0.00773298	norm: 0.06377148
----------------0-----------
x:  0.0 3.816488970187493e-06
v:  -1.0338774814044882e-07 -9.312758919577391e-08
p:  -1.1920928955078125e-07 -8.940696716308594e-08 7.993605777301127e-15 1.4210854715202004e-14
d:  1.5987211554602254e-14 2.842170943040401e-14 
k:  0.022097086533904076 0.022097086533904076 
----------------1-----------
x:  0.0 0.00020702878828160465
v:  -1.0338224853967404e-07 -9.310710424870194e-08
p:  -2.384185791015625e-07 -1.7881393432617188e-07 3.197442310920451e-14 5.684341886080802e-14
d:  6.394884621840902e-14 1.1368683772161603e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------2-----------
x:  0.0 0.00020702350593637675
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -3.5762786865234375e-07 -2.682209014892578e-07 7.194245199571014e-14 1.2789769243681803e-13
d:  1.438849039914203e-13 2.5579538487363607e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------3-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -4.76837158203125e-07 -3.5762786865234375e-07 1.2789769243681803e-13 2.2737367544323206e-13
d:  2.5579538487363607e-13 4.547473508864641e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------4-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -5.960464477539062e-07 -4.470348358154297e-07 1.9984014443252818e-13 3.552713678800501e-13
d:  3.9968028886505635e-13 7.105427357601002e-13 
k:  0.022097086533904076 0.022097086533904076 
----------------5-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -7.152557373046875e-07 -5.364418029785156e-07 2.877698079828406e-13 5.115907697472721e-13
d:  5.755396159656812e-13 1.0231815394945443e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------6-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -8.344650268554688e-07 -6.258487701416016e-07 3.9168668308775523e-13 6.963318810448982e-13
d:  7.833733661755105e-13 1.3926637620897964e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------7-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -9.5367431640625e-07 -7.152557373046875e-07 5.115907697472721e-13 9.094947017729282e-13
d:  1.0231815394945443e-12 1.8189894035458565e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------8-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -1.0728836059570312e-06 -8.046627044677734e-07 6.474820679613913e-13 1.1510792319313623e-12
d:  1.2949641359227826e-12 2.3021584638627246e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------9-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -1.1920928955078125e-06 -8.940696716308594e-07 7.993605777301127e-13 1.4210854715202004e-12
d:  1.5987211554602254e-12 2.8421709430404007e-12 
k:  0.022097086533904076 0.022097086533904076 
----------------10-----------
x:  0.0 0.00020702346228063107
v:  -1.0338224853967404e-07 -9.310711845955666e-08
p:  -1.3113021850585938e-06 -9.834766387939453e-07 9.672262990534364e-13 1.7195134205394424e-12
d:  1.9344525981068728e-12 3.439026841078885e-12 
k:  0.022097086533904076 0.022097086533904076 
